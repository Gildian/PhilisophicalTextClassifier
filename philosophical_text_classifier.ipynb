{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4b8efb",
   "metadata": {},
   "source": [
    "# BERT Philosophical Text Classifier\n",
    "\n",
    "## Continental vs Analytic Philosophy Style Detection\n",
    "\n",
    "This notebook implements a BERT-based deep learning model to classify philosophical texts as either **Continental** or **Analytic** philosophy styles. The model analyzes the linguistic patterns, terminology, and stylistic features that distinguish these two major philosophical traditions.\n",
    "\n",
    "### What You'll Learn:\n",
    "- How to fine-tune BERT for text classification\n",
    "- Distinguishing features of Continental vs Analytic philosophy\n",
    "- Building a complete ML pipeline from data preparation to prediction\n",
    "- Evaluating model performance and visualizing results\n",
    "\n",
    "### Philosophical Traditions Overview:\n",
    "\n",
    "**Continental Philosophy** is characterized by:\n",
    "- Emphasis on lived experience and historical context\n",
    "- Phenomenological and hermeneutical approaches  \n",
    "- Focus on existential and dialectical themes\n",
    "- Key figures: Heidegger, Sartre, Derrida, Foucault\n",
    "\n",
    "**Analytic Philosophy** is characterized by:\n",
    "- Logical rigor and formal analysis\n",
    "- Emphasis on conceptual clarity and precision\n",
    "- Problem-solving approach using formal methods\n",
    "- Key figures: Russell, Quine, Davidson, Kripke\n",
    "\n",
    "Let's build a classifier that can distinguish between these styles!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de69ded",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our BERT-based philosophical text classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880be2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers and NLP\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d812b56",
   "metadata": {},
   "source": [
    "## 2. Define the BERT Classifier Model\n",
    "\n",
    "Now let's create our BERT-based classifier. This model will:\n",
    "- Use a pre-trained BERT model as the base\n",
    "- Add a classification head with dropout for regularization\n",
    "- Output probability scores for Continental vs Analytic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9947384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhilosophicalBERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT-based classifier for distinguishing between Continental and Analytic philosophy texts.\n",
    "    \n",
    "    Architecture:\n",
    "    - Pre-trained BERT model for feature extraction\n",
    "    - Dropout layer for regularization  \n",
    "    - Linear classification layer\n",
    "    - Softmax activation for probability outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'bert-base-uncased', num_classes: int = 2, dropout_rate: float = 0.3):\n",
    "        super(PhilosophicalBERTClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained BERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze first few layers to retain pre-trained knowledge\n",
    "        # We'll fine-tune the last few layers\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        for layer in self.bert.encoder.layer[:8]:  # Freeze first 8 layers\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        print(f\"Model initialized with {model_name}\")\n",
    "        print(f\"Hidden size: {self.bert.config.hidden_size}\")\n",
    "        print(f\"Number of classes: {num_classes}\")\n",
    "        print(f\"Dropout rate: {dropout_rate}\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Token IDs from BERT tokenizer\n",
    "            attention_mask: Attention mask to ignore padding tokens\n",
    "            \n",
    "        Returns:\n",
    "            logits: Raw classification scores\n",
    "            probabilities: Softmax probabilities for each class\n",
    "        \"\"\"\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use pooled output (CLS token representation)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Classification layer\n",
    "        logits = self.classifier(output)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = self.softmax(logits)\n",
    "        \n",
    "        return logits, probabilities\n",
    "\n",
    "# Test model instantiation\n",
    "test_model = PhilosophicalBERTClassifier()\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in test_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Clean up test model\n",
    "del test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e87b2",
   "metadata": {},
   "source": [
    "## 3. Create the Philosophy Dataset Class\n",
    "\n",
    "We need a custom Dataset class to handle the preprocessing of philosophical texts for BERT input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhilosophyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for philosophical texts.\n",
    "    \n",
    "    This class handles:\n",
    "    - Text tokenization using BERT tokenizer\n",
    "    - Proper padding and truncation\n",
    "    - Label encoding for classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], labels: List[str], tokenizer, max_length: int = 512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Create label mapping\n",
    "        self.label_to_idx = {'Continental': 0, 'Analytic': 1}\n",
    "        self.idx_to_label = {0: 'Continental', 1: 'Analytic'}\n",
    "        \n",
    "        print(f\"Dataset created with {len(texts)} samples\")\n",
    "        print(f\"Max sequence length: {max_length}\")\n",
    "        print(f\"Label mapping: {self.label_to_idx}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single item from the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "            - input_ids: Token IDs for BERT\n",
    "            - attention_mask: Mask for padding tokens\n",
    "            - labels: Encoded label (0=Continental, 1=Analytic)\n",
    "        \"\"\"\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Encode the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.label_to_idx[label], dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def get_label_distribution(self):\n",
    "        \"\"\"Get the distribution of labels in the dataset.\"\"\"\n",
    "        from collections import Counter\n",
    "        label_counts = Counter(self.labels)\n",
    "        return label_counts\n",
    "\n",
    "# Test the dataset class with sample data\n",
    "sample_texts = [\n",
    "    \"Being-in-the-world reveals the fundamental structure of human existence.\",\n",
    "    \"The principle of charity requires interpreting arguments in their strongest form.\"\n",
    "]\n",
    "sample_labels = [\"Continental\", \"Analytic\"]\n",
    "\n",
    "# Initialize tokenizer for testing\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = PhilosophyDataset(sample_texts, sample_labels, tokenizer, max_length=128)\n",
    "\n",
    "# Test getting an item\n",
    "sample_item = test_dataset[0]\n",
    "print(f\"\\nSample item structure:\")\n",
    "for key, value in sample_item.items():\n",
    "    print(f\"{key}: {value.shape if hasattr(value, 'shape') else type(value)}\")\n",
    "\n",
    "print(f\"\\nLabel distribution: {test_dataset.get_label_distribution()}\")\n",
    "\n",
    "# Clean up\n",
    "del test_dataset, sample_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572d8eb",
   "metadata": {},
   "source": [
    "## 4. Initialize the Main Classifier Class\n",
    "\n",
    "Let's create the main class that orchestrates the entire pipeline for training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa298c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhilosophyClassifier:\n",
    "    \"\"\"\n",
    "    Main classifier class that handles the complete pipeline for philosophical text classification.\n",
    "    \n",
    "    Features:\n",
    "    - Model initialization and configuration\n",
    "    - Data preparation and preprocessing\n",
    "    - Training with validation\n",
    "    - Prediction with probability outputs\n",
    "    - Model saving and loading\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'bert-base-uncased', max_length: int = 512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Model will be initialized during training\n",
    "        self.model = None\n",
    "        \n",
    "        # Class mapping\n",
    "        self.class_names = ['Continental', 'Analytic']\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}\n",
    "        self.idx_to_class = {idx: name for idx, name in enumerate(self.class_names)}\n",
    "        \n",
    "        print(f\"PhilosophyClassifier initialized:\")\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        print(f\"  Max length: {max_length}\")\n",
    "        print(f\"  Device: {self.device}\")\n",
    "        print(f\"  Classes: {self.class_names}\")\n",
    "        \n",
    "    def prepare_data(self, texts: List[str], labels: List[str], test_size: float = 0.2, batch_size: int = 8):\n",
    "        \"\"\"\n",
    "        Prepare data loaders for training and validation.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of philosophical texts\n",
    "            labels: List of corresponding labels ('Continental' or 'Analytic')\n",
    "            test_size: Fraction of data to use for validation\n",
    "            batch_size: Batch size for training\n",
    "            \n",
    "        Returns:\n",
    "            train_loader, val_loader: PyTorch DataLoaders\n",
    "        \"\"\"\n",
    "        print(f\"Preparing data with {len(texts)} samples...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            texts, labels, test_size=test_size, random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        print(f\"Train samples: {len(train_texts)}\")\n",
    "        print(f\"Validation samples: {len(val_texts)}\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = PhilosophyDataset(train_texts, train_labels, self.tokenizer, self.max_length)\n",
    "        val_dataset = PhilosophyDataset(val_texts, val_labels, self.tokenizer, self.max_length)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f\"Data loaders created with batch size: {batch_size}\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def predict_single(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Predict the philosophical style of a single text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input philosophical text\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with probability scores for each class\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Please train the model first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Preprocess text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, probabilities = self.model(\n",
    "                encoding['input_ids'], \n",
    "                encoding['attention_mask']\n",
    "            )\n",
    "            probs = probabilities.cpu().numpy()[0]\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result = {}\n",
    "        for idx, prob in enumerate(probs):\n",
    "            class_name = self.idx_to_class[idx]\n",
    "            result[class_name] = float(prob)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the trained model and configuration.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save. Train the model first.\")\n",
    "        \n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'model_name': self.model_name,\n",
    "            'max_length': self.max_length,\n",
    "            'class_names': self.class_names,\n",
    "            'class_to_idx': self.class_to_idx,\n",
    "            'idx_to_class': self.idx_to_class\n",
    "        }\n",
    "        \n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load a trained model from disk.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        \n",
    "        self.model = PhilosophicalBERTClassifier(checkpoint['model_name']).to(self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        self.model_name = checkpoint['model_name']\n",
    "        self.max_length = checkpoint['max_length']\n",
    "        self.class_names = checkpoint['class_names']\n",
    "        self.class_to_idx = checkpoint['class_to_idx']\n",
    "        self.idx_to_class = checkpoint['idx_to_class']\n",
    "        \n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = PhilosophyClassifier()\n",
    "print(\"\\\\nClassifier ready for data preparation and training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c89658",
   "metadata": {},
   "source": [
    "## 5. Generate Sample Philosophical Data\n",
    "\n",
    "Let's create a comprehensive dataset with representative texts from both Continental and Analytic philosophy traditions. These samples showcase the distinctive linguistic patterns and conceptual approaches of each tradition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_philosophical_dataset():\n",
    "    \"\"\"\n",
    "    Create a comprehensive dataset of philosophical texts from both traditions.\n",
    "    \n",
    "    Continental Philosophy Features:\n",
    "    - Phenomenological terminology (Dasein, Being-in-the-world, etc.)\n",
    "    - Existential themes (anxiety, authenticity, freedom)\n",
    "    - Dialectical thinking and historical consciousness\n",
    "    - Emphasis on lived experience and interpretation\n",
    "    \n",
    "    Analytic Philosophy Features:\n",
    "    - Logical precision and formal analysis\n",
    "    - Epistemological problems (knowledge, justification)\n",
    "    - Conceptual analysis and linguistic clarity\n",
    "    - Problem-solving approach with rigorous arguments\n",
    "    \"\"\"\n",
    "    \n",
    "    # Continental Philosophy Texts\n",
    "    continental_texts = [\n",
    "        # Heidegger-inspired existential analysis\n",
    "        \"Being-in-the-world is a fundamental structure of Dasein that reveals the primordial unity of our existence. The phenomenon of anxiety discloses the nothingness that underlies all beings, showing us the groundlessness of our thrown existence.\",\n",
    "        \n",
    "        # Hegelian dialectical philosophy\n",
    "        \"The dialectical movement of history unfolds through the negation of negation, where consciousness encounters its other and returns to itself transformed. This process of Bildung constitutes the very essence of Spirit's self-realization.\",\n",
    "        \n",
    "        # Heidegger on language and Being\n",
    "        \"Language is the house of Being, and in its home dwells man. When we go to the well, when we go through the woods, we are always already going through the word 'well,' through the word 'woods,' even if we do not speak these words out loud or think of anything relating to language.\",\n",
    "        \n",
    "        # Levinasian ethics\n",
    "        \"The Other precedes the Same, and responsibility for the Other is prior to freedom. The face-to-face encounter reveals the infinite responsibility that constitutes subjectivity itself.\",\n",
    "        \n",
    "        # Sartrean existentialism\n",
    "        \"Existence precedes essence, and man is condemned to be free. In anguish, we confront the radical contingency of our situation and the weight of absolute responsibility for our choices.\",\n",
    "        \n",
    "        # Husserlian phenomenology\n",
    "        \"The lifeworld provides the horizon of meaning within which all scientific abstractions find their ultimate foundation. The crisis of the European sciences stems from the forgetting of this fundamental stratum of experience.\",\n",
    "        \n",
    "        # Derridean deconstruction\n",
    "        \"Difference differs from itself, creating the play of signification that undermines any metaphysics of presence. The trace structure of meaning reveals the impossibility of pure self-presence.\",\n",
    "        \n",
    "        # Foucauldian analysis\n",
    "        \"Power produces knowledge, and knowledge reinforces power relations. The subject is not the sovereign author of discourse but is constituted through discursive practices and technologies of the self.\",\n",
    "        \n",
    "        # Lacanian psychoanalysis\n",
    "        \"The Imaginary, Symbolic, and Real form the three registers that structure human subjectivity. The unconscious is structured like a language, and desire circulates through the signifying chain.\",\n",
    "        \n",
    "        # Phenomenological analysis\n",
    "        \"Lived experience (Erlebnis) differs fundamentally from mere occurrence, as it carries within itself the intentional structure that connects consciousness to its objects through various modes of givenness.\",\n",
    "        \n",
    "        # Gadamerian hermeneutics\n",
    "        \"Understanding is not a methodological procedure but the fundamental mode of human being-in-the-world. The fusion of horizons occurs when the interpreter's prejudices encounter the tradition's claims to truth.\",\n",
    "        \n",
    "        # Merleau-Ponty on embodiment\n",
    "        \"The body is not merely an object among objects but the very condition of possibility for experiencing a world. Perception is not a mental act but a bodily engagement with the sensible world.\",\n",
    "        \n",
    "        # Adorno on negative dialectics\n",
    "        \"The concept's totalizing tendency must be broken by its own contradictions. Negative dialectics preserves the non-identical by insisting on the primacy of the object against conceptual domination.\",\n",
    "        \n",
    "        # Benjamin on historical materialism\n",
    "        \"The angel of history sees one single catastrophe which keeps piling wreckage upon wreckage. What we call progress is this storm blowing from Paradise, piling up the debris of the past.\",\n",
    "        \n",
    "        # Bataille on excess and expenditure\n",
    "        \"The accursed share reveals the fundamental principle of loss that governs all economies. Human existence is characterized by the need to destroy or waste the excess energy that cannot be accumulated.\",\n",
    "    ]\n",
    "    \n",
    "    # Analytic Philosophy Texts\n",
    "    analytic_texts = [\n",
    "        # Gettier problem in epistemology\n",
    "        \"If knowledge is justified true belief, then the Gettier problem shows that justification alone is insufficient. We need an additional condition that prevents the kind of epistemic luck that makes justified true belief fall short of knowledge.\",\n",
    "        \n",
    "        # Principle of charity in interpretation\n",
    "        \"The principle of charity requires that we interpret others' arguments in their strongest form. This methodological constraint ensures that philosophical dialogue proceeds through genuine engagement with opposing positions rather than straw man attacks.\",\n",
    "        \n",
    "        # David Lewis on modal realism\n",
    "        \"Modal realism holds that all possible worlds exist as concrete physical realities. This thesis, while counterintuitive, provides elegant solutions to problems about the nature of properties, propositions, and possibility itself.\",\n",
    "        \n",
    "        # Kripke on rigid designation\n",
    "        \"The causal theory of reference establishes that proper names are rigid designators whose reference is fixed through causal chains leading back to initial baptismal events. This explains how names can refer even when speakers lack definite descriptions.\",\n",
    "        \n",
    "        # Functionalism in philosophy of mind\n",
    "        \"Functionalism defines mental states in terms of their causal roles rather than their physical realizations. Pain, for instance, is whatever state typically causes withdrawal behavior and is caused by tissue damage, regardless of its material substrate.\",\n",
    "        \n",
    "        # Hume's is-ought problem\n",
    "        \"The is-ought problem demonstrates that normative conclusions cannot be validly derived from purely descriptive premises. This logical gap requires additional normative assumptions to bridge the transition from facts to values.\",\n",
    "        \n",
    "        # Quine on indeterminacy of translation\n",
    "        \"Quine's indeterminacy thesis shows that translation between languages is underdetermined by all possible behavioral evidence. This result undermines the notion of determinate meaning and supports a holistic view of language.\",\n",
    "        \n",
    "        # Jackson's knowledge argument\n",
    "        \"The knowledge argument purports to show that physicalism is false because Mary learns new facts upon experiencing color for the first time. However, the ability hypothesis suggests she gains abilities rather than propositional knowledge.\",\n",
    "        \n",
    "        # Putnam's Twin Earth argument\n",
    "        \"Externalism about mental content holds that the environment partially determines what we think. Twin Earth scenarios demonstrate that molecularly identical beings can have different thoughts if their environments differ.\",\n",
    "        \n",
    "        # Problem of induction\n",
    "        \"The problem of induction challenges the rational justification for inductive reasoning. Even if past instances of bread nourished us, logic alone cannot establish that future bread will continue to do so.\",\n",
    "        \n",
    "        # Russell's theory of descriptions\n",
    "        \"Definite descriptions are not genuine singular terms but rather quantified expressions. The sentence 'The present King of France is bald' is false rather than lacking a truth value, because it contains a false existential presupposition.\",\n",
    "        \n",
    "        # Tarski on truth\n",
    "        \"The semantic conception of truth provides a formally adequate definition within a metalanguage. For any sentence S in the object language, 'S is true' if and only if S, where the right-hand occurrence of S is used rather than mentioned.\",\n",
    "        \n",
    "        # Davidson on radical interpretation\n",
    "        \"Radical interpretation requires the simultaneous assignment of beliefs and meanings to maximize agreement with the interpreter's own beliefs. The principle of charity is not optional but constitutive of interpretation itself.\",\n",
    "        \n",
    "        # Searle's Chinese Room argument\n",
    "        \"The Chinese Room argument demonstrates that syntax is insufficient for semantics. A system can manipulate symbols according to formal rules without understanding their meaning, showing that computation alone cannot account for intentionality.\",\n",
    "        \n",
    "        # Lewis on laws of nature\n",
    "        \"Laws of nature are regularities that hold in all possible worlds where the Humean mosaic of particular facts achieves the best balance of simplicity and strength. This analysis reduces laws to patterns in the distribution of local qualities.\",\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    all_texts = continental_texts + analytic_texts\n",
    "    all_labels = ['Continental'] * len(continental_texts) + ['Analytic'] * len(analytic_texts)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': all_texts,\n",
    "        'label': all_labels,\n",
    "        'text_length': [len(text) for text in all_texts],\n",
    "        'word_count': [len(text.split()) for text in all_texts]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the dataset\n",
    "philosophy_df = create_philosophical_dataset()\n",
    "\n",
    "print(f\"Dataset created with {len(philosophy_df)} samples\")\n",
    "print(f\"\\\\nLabel distribution:\")\n",
    "print(philosophy_df['label'].value_counts())\n",
    "\n",
    "print(f\"\\\\nText statistics:\")\n",
    "print(f\"Average text length: {philosophy_df['text_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {philosophy_df['word_count'].mean():.1f} words\")\n",
    "print(f\"Max text length: {philosophy_df['text_length'].max()} characters\")\n",
    "print(f\"Min text length: {philosophy_df['text_length'].min()} characters\")\n",
    "\n",
    "# Display sample texts\n",
    "print(f\"\\\\n=== Sample Continental Text ===\")\n",
    "continental_sample = philosophy_df[philosophy_df['label'] == 'Continental'].iloc[0]\n",
    "print(f\"Text: {continental_sample['text']}\")\n",
    "print(f\"Length: {continental_sample['text_length']} chars, {continental_sample['word_count']} words\")\n",
    "\n",
    "print(f\"\\\\n=== Sample Analytic Text ===\")\n",
    "analytic_sample = philosophy_df[philosophy_df['label'] == 'Analytic'].iloc[0]\n",
    "print(f\"Text: {analytic_sample['text']}\")\n",
    "print(f\"Length: {analytic_sample['text_length']} chars, {analytic_sample['word_count']} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd2ac0",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for Training\n",
    "\n",
    "Now let's prepare our data for training by creating train/validation splits and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "texts = philosophy_df['text'].tolist()\n",
    "labels = philosophy_df['label'].tolist()\n",
    "\n",
    "# Create train/validation data loaders\n",
    "train_loader, val_loader = classifier.prepare_data(\n",
    "    texts=texts,\n",
    "    labels=labels,\n",
    "    test_size=0.2,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Examine a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\\\nSample batch structure:\")\n",
    "for key, value in sample_batch.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "# Check tokenizer vocabulary size\n",
    "print(f\"\\\\nTokenizer info:\")\n",
    "print(f\"  Vocabulary size: {len(classifier.tokenizer)}\")\n",
    "print(f\"  Model max length: {classifier.tokenizer.model_max_length}\")\n",
    "print(f\"  Special tokens: {classifier.tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Decode a sample to verify tokenization\n",
    "sample_input_ids = sample_batch['input_ids'][0]\n",
    "decoded_text = classifier.tokenizer.decode(sample_input_ids, skip_special_tokens=True)\n",
    "print(f\"\\\\nSample decoded text (first 100 chars): {decoded_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e3e6a",
   "metadata": {},
   "source": [
    "## 7. Train the BERT Model\n",
    "\n",
    "Now let's implement the training loop to fine-tune BERT for our philosophical text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_philosophy_classifier(classifier, train_loader, val_loader, epochs=3, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Train the BERT classifier for philosophical text classification.\n",
    "    \n",
    "    Args:\n",
    "        classifier: PhilosophyClassifier instance\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "        training_history: Dictionary with training metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize model\n",
    "    classifier.model = PhilosophicalBERTClassifier(classifier.model_name).to(classifier.device)\n",
    "    \n",
    "    # Setup optimizer and loss function\n",
    "    optimizer = optim.AdamW(classifier.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Device: {classifier.device}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = datetime.now()\n",
    "        \n",
    "        # Training phase\n",
    "        classifier.model.train()\n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(classifier.device)\n",
    "            attention_mask = batch['attention_mask'].to(classifier.device)\n",
    "            labels = batch['labels'].to(classifier.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = classifier.model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "            \n",
    "            # Print progress every 5 batches\n",
    "            if (batch_idx + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        classifier.model.eval()\n",
    "        total_val_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        val_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(classifier.device)\n",
    "                attention_mask = batch['attention_mask'].to(classifier.device)\n",
    "                labels = batch['labels'].to(classifier.device)\n",
    "                \n",
    "                logits, probabilities = classifier.model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "                \n",
    "                # Get predictions\n",
    "                predictions = torch.argmax(probabilities, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_val_loss = total_val_loss / val_steps\n",
    "        val_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Store metrics\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print epoch summary\n",
    "        epoch_time = datetime.now() - epoch_start_time\n",
    "        print(f\"\\\\nEpoch {epoch+1}/{epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Val F1: {val_f1:.4f}\")\n",
    "        print(f\"  Time: {epoch_time}\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "training_history = train_philosophy_classifier(\n",
    "    classifier=classifier,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=3,\n",
    "    learning_rate=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d674c97",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model Performance\n",
    "\n",
    "Let's evaluate our trained model and create detailed performance reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15dbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(classifier, val_loader):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the trained model.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with detailed evaluation metrics\n",
    "    \"\"\"\n",
    "    classifier.model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(classifier.device)\n",
    "            attention_mask = batch['attention_mask'].to(classifier.device)\n",
    "            labels = batch['labels'].to(classifier.device)\n",
    "            \n",
    "            logits, probabilities = classifier.model(input_ids, attention_mask)\n",
    "            \n",
    "            predictions = torch.argmax(probabilities, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average=None\n",
    "    )\n",
    "    \n",
    "    # Overall weighted metrics\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_per_class': precision,\n",
    "        'recall_per_class': recall,\n",
    "        'f1_per_class': f1,\n",
    "        'support_per_class': support,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_predictions,\n",
    "        'probabilities': all_probabilities,\n",
    "        'true_labels': all_labels\n",
    "    }\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_model(classifier, val_loader)\n",
    "\n",
    "print(\"=== Model Performance Evaluation ===\\\\n\")\n",
    "\n",
    "print(f\"Overall Accuracy: {evaluation_results['accuracy']:.4f}\")\n",
    "print(f\"Weighted Precision: {evaluation_results['precision_weighted']:.4f}\")\n",
    "print(f\"Weighted Recall: {evaluation_results['recall_weighted']:.4f}\")\n",
    "print(f\"Weighted F1-Score: {evaluation_results['f1_weighted']:.4f}\")\n",
    "\n",
    "print(\"\\\\n=== Per-Class Performance ===\")\n",
    "class_names = ['Continental', 'Analytic']\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"\\\\n{class_name}:\")\n",
    "    print(f\"  Precision: {evaluation_results['precision_per_class'][i]:.4f}\")\n",
    "    print(f\"  Recall: {evaluation_results['recall_per_class'][i]:.4f}\")\n",
    "    print(f\"  F1-Score: {evaluation_results['f1_per_class'][i]:.4f}\")\n",
    "    print(f\"  Support: {evaluation_results['support_per_class'][i]}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\\\n=== Confusion Matrix ===\")\n",
    "cm = evaluation_results['confusion_matrix']\n",
    "print(f\"\\\\n{'':>12} {'Continental':>12} {'Analytic':>12}\")\n",
    "print(f\"{'Continental':>12} {cm[0,0]:>12} {cm[0,1]:>12}\")\n",
    "print(f\"{'Analytic':>12} {cm[1,0]:>12} {cm[1,1]:>12}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\\\n=== Detailed Classification Report ===\")\n",
    "print(classification_report(\n",
    "    evaluation_results['true_labels'], \n",
    "    evaluation_results['predictions'], \n",
    "    target_names=class_names\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8aa5a",
   "metadata": {},
   "source": [
    "## 9. Implement Text Prediction Function\n",
    "\n",
    "Now let's create a user-friendly prediction function that takes any philosophical text and returns probability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_philosophical_style(text, classifier, detailed=True):\n",
    "    \"\"\"\n",
    "    Predict the philosophical style of input text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input philosophical text to classify\n",
    "        classifier: Trained PhilosophyClassifier instance\n",
    "        detailed: Whether to return detailed analysis\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with prediction results and probabilities\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return {\"error\": \"Please provide non-empty text\"}\n",
    "    \n",
    "    try:\n",
    "        # Get prediction\n",
    "        result = classifier.predict_single(text)\n",
    "        \n",
    "        # Determine predicted class\n",
    "        predicted_class = max(result, key=result.get)\n",
    "        confidence = result[predicted_class]\n",
    "        \n",
    "        # Basic result\n",
    "        prediction_result = {\n",
    "            \"input_text\": text,\n",
    "            \"predicted_style\": predicted_class,\n",
    "            \"confidence\": confidence,\n",
    "            \"probabilities\": {\n",
    "                \"Continental\": result[\"Continental\"],\n",
    "                \"Analytic\": result[\"Analytic\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if detailed:\n",
    "            # Add detailed analysis\n",
    "            continental_prob = result[\"Continental\"]\n",
    "            analytic_prob = result[\"Analytic\"]\n",
    "            \n",
    "            # Style characteristics\n",
    "            if predicted_class == \"Continental\":\n",
    "                characteristics = [\n",
    "                    \"🏛️ Emphasis on lived experience and historical context\",\n",
    "                    \"🌊 Dialectical and phenomenological approach\", \n",
    "                    \"📖 Focus on interpretation and hermeneutics\",\n",
    "                    \"🎭 Existential and ontological themes\",\n",
    "                    \"🌀 Critique of traditional metaphysics\"\n",
    "                ]\n",
    "            else:\n",
    "                characteristics = [\n",
    "                    \"🔬 Logical rigor and formal analysis\",\n",
    "                    \"🎯 Conceptual clarity and precision\",\n",
    "                    \"⚖️ Problem-solving methodology\",\n",
    "                    \"📊 Empirical and scientific approach\",\n",
    "                    \"🔍 Language and meaning analysis\"\n",
    "                ]\n",
    "            \n",
    "            prediction_result.update({\n",
    "                \"detailed_analysis\": {\n",
    "                    \"continental_percentage\": f\"{continental_prob*100:.1f}%\",\n",
    "                    \"analytic_percentage\": f\"{analytic_prob*100:.1f}%\",\n",
    "                    \"confidence_level\": \"High\" if confidence > 0.8 else \"Medium\" if confidence > 0.6 else \"Low\",\n",
    "                    \"style_characteristics\": characteristics,\n",
    "                    \"text_length\": len(text),\n",
    "                    \"word_count\": len(text.split())\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return prediction_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Prediction failed: {str(e)}\"}\n",
    "\n",
    "def display_prediction(result):\n",
    "    \"\"\"Display prediction results in a formatted way.\"\"\"\n",
    "    if \"error\" in result:\n",
    "        print(f\"❌ Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(\"🎓 PHILOSOPHICAL STYLE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"📝 Input Text:\")\n",
    "    text = result[\"input_text\"]\n",
    "    if len(text) > 150:\n",
    "        print(f\"   {text[:150]}...\")\n",
    "    else:\n",
    "        print(f\"   {text}\")\n",
    "    \n",
    "    print(f\"\\\\n🎯 Prediction: {result['predicted_style']}\")\n",
    "    print(f\"🔍 Confidence: {result['confidence']:.1%}\")\n",
    "    \n",
    "    print(f\"\\\\n📊 Probability Breakdown:\")\n",
    "    for style, prob in result[\"probabilities\"].items():\n",
    "        bar_length = int(prob * 20)  # Scale to 20 characters\n",
    "        bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "        print(f\"   {style:12} {bar} {prob:.1%}\")\n",
    "    \n",
    "    if \"detailed_analysis\" in result:\n",
    "        details = result[\"detailed_analysis\"]\n",
    "        print(f\"\\\\n🎨 Style Characteristics:\")\n",
    "        for char in details[\"style_characteristics\"]:\n",
    "            print(f\"   {char}\")\n",
    "        \n",
    "        print(f\"\\\\n📏 Text Statistics:\")\n",
    "        print(f\"   Length: {details['text_length']} characters\")\n",
    "        print(f\"   Words: {details['word_count']}\")\n",
    "        print(f\"   Confidence Level: {details['confidence_level']}\")\n",
    "\n",
    "# Test the prediction function with a sample text\n",
    "test_text = \"Being-in-the-world is a fundamental structure of Dasein that reveals the primordial unity of our existence.\"\n",
    "\n",
    "print(\"Testing prediction function...\")\n",
    "sample_prediction = predict_philosophical_style(test_text, classifier)\n",
    "display_prediction(sample_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da4b0b",
   "metadata": {},
   "source": [
    "## 10. Test the Classifier with Sample Texts\n",
    "\n",
    "Let's test our classifier with various philosophical texts from famous philosophers to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test texts from famous philosophers\n",
    "test_texts = {\n",
    "    \"Heidegger (Continental)\": \"The essence of Dasein lies in its existence. Dasein is that entity which is in each case mine, and which has, as its manner of Being, the possibility of existing authentically or inauthentically.\",\n",
    "    \n",
    "    \"Sartre (Continental)\": \"Man is condemned to be free; because once thrown into the world, he is responsible for everything he does. It is up to you to give life a meaning.\",\n",
    "    \n",
    "    \"Derrida (Continental)\": \"Every sign, linguistic or nonlinguistic, spoken or written, in a small or large unit, can be cited, grafted, iterated. This iterability alters, parasitically, the identity of the element.\",\n",
    "    \n",
    "    \"Russell (Analytic)\": \"The method of 'postulating' what we want has many advantages; they are the same as the advantages of theft over honest toil. Let us see what can be done with conscientious toil.\",\n",
    "    \n",
    "    \"Wittgenstein (Analytic)\": \"The limits of my language mean the limits of my world. Whereof one cannot speak, thereof one must be silent.\",\n",
    "    \n",
    "    \"Quine (Analytic)\": \"The totality of our so-called knowledge or beliefs, from the most casual matters of geography and history to the profoundest laws of atomic physics, is a man-made fabric which impinges on experience only along the edges.\",\n",
    "    \n",
    "    \"Foucault (Continental)\": \"Where there is power, there is resistance, and yet, or rather consequently, this resistance is never in a position of exteriority in relation to power.\",\n",
    "    \n",
    "    \"Davidson (Analytic)\": \"In giving up the dualism of scheme and world, we do not give up the world, but reestablish unmediated touch with the familiar objects whose antics make our sentences and opinions true or false.\",\n",
    "    \n",
    "    \"Merleau-Ponty (Continental)\": \"The body is our general medium for having a world. Sometimes it is restricted to the actions necessary for the conservation of life, and accordingly it posits around us a biological world.\",\n",
    "    \n",
    "    \"Kripke (Analytic)\": \"A rigid designator designates the same object in all possible worlds in which that object exists and never designates anything else.\"\n",
    "}\n",
    "\n",
    "print(\"🧪 TESTING CLASSIFIER WITH FAMOUS PHILOSOPHERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test each text and store results\n",
    "test_results = []\n",
    "\n",
    "for philosopher, text in test_texts.items():\n",
    "    print(f\"\\\\n📚 Testing: {philosopher}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = predict_philosophical_style(text, classifier, detailed=False)\n",
    "    \n",
    "    if \"error\" not in result:\n",
    "        predicted = result[\"predicted_style\"]\n",
    "        confidence = result[\"confidence\"]\n",
    "        continental_prob = result[\"probabilities\"][\"Continental\"]\n",
    "        analytic_prob = result[\"probabilities\"][\"Analytic\"]\n",
    "        \n",
    "        # Extract expected style from label\n",
    "        expected = \"Continental\" if \"Continental\" in philosopher else \"Analytic\"\n",
    "        correct = predicted == expected\n",
    "        \n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Predicted: {predicted} ({confidence:.1%} confidence)\")\n",
    "        print(f\"Continental: {continental_prob:.1%} | Analytic: {analytic_prob:.1%}\")\n",
    "        print(f\"✅ Correct\" if correct else \"❌ Incorrect\")\n",
    "        \n",
    "        test_results.append({\n",
    "            'philosopher': philosopher,\n",
    "            'expected': expected,\n",
    "            'predicted': predicted,\n",
    "            'correct': correct,\n",
    "            'confidence': confidence,\n",
    "            'continental_prob': continental_prob,\n",
    "            'analytic_prob': analytic_prob\n",
    "        })\n",
    "    else:\n",
    "        print(f\"❌ Error: {result['error']}\")\n",
    "\n",
    "# Calculate overall accuracy on test samples\n",
    "if test_results:\n",
    "    correct_predictions = sum(1 for r in test_results if r['correct'])\n",
    "    total_predictions = len(test_results)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"\\\\n\\\\n📊 OVERALL TEST RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Accuracy: {accuracy:.1%} ({correct_predictions}/{total_predictions})\")\n",
    "    \n",
    "    # Breakdown by expected class\n",
    "    continental_results = [r for r in test_results if r['expected'] == 'Continental']\n",
    "    analytic_results = [r for r in test_results if r['expected'] == 'Analytic']\n",
    "    \n",
    "    if continental_results:\n",
    "        continental_accuracy = sum(1 for r in continental_results if r['correct']) / len(continental_results)\n",
    "        avg_continental_confidence = sum(r['confidence'] for r in continental_results if r['correct']) / max(1, sum(1 for r in continental_results if r['correct']))\n",
    "        print(f\"Continental accuracy: {continental_accuracy:.1%}\")\n",
    "        print(f\"Avg confidence (correct): {avg_continental_confidence:.1%}\")\n",
    "    \n",
    "    if analytic_results:\n",
    "        analytic_accuracy = sum(1 for r in analytic_results if r['correct']) / len(analytic_results)\n",
    "        avg_analytic_confidence = sum(r['confidence'] for r in analytic_results if r['correct']) / max(1, sum(1 for r in analytic_results if r['correct']))\n",
    "        print(f\"Analytic accuracy: {analytic_accuracy:.1%}\")\n",
    "        print(f\"Avg confidence (correct): {avg_analytic_confidence:.1%}\")\n",
    "    \n",
    "    # Show misclassified examples\n",
    "    misclassified = [r for r in test_results if not r['correct']]\n",
    "    if misclassified:\n",
    "        print(f\"\\\\n❌ Misclassified examples:\")\n",
    "        for r in misclassified:\n",
    "            print(f\"  {r['philosopher']}: Expected {r['expected']}, got {r['predicted']} ({r['confidence']:.1%})\")\n",
    "\n",
    "print(f\"\\\\n\\\\n🎯 Try your own text!\")\n",
    "print(\"Use: predict_philosophical_style('your text here', classifier)\")\n",
    "print(\"Or:  display_prediction(predict_philosophical_style('your text', classifier))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a772865",
   "metadata": {},
   "source": [
    "## 11. Visualize Training Results\n",
    "\n",
    "Let's create comprehensive visualizations of our training process and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed2e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('BERT Philosophical Text Classifier - Training & Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "epochs = range(1, len(training_history['train_loss']) + 1)\n",
    "axes[0, 0].plot(epochs, training_history['train_loss'], 'b-o', label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(epochs, training_history['val_loss'], 'r-s', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation Accuracy\n",
    "axes[0, 1].plot(epochs, training_history['val_accuracy'], 'g-^', label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Validation Accuracy Progress', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add accuracy percentage labels\n",
    "for i, acc in enumerate(training_history['val_accuracy']):\n",
    "    axes[0, 1].annotate(f'{acc:.1%}', (i+1, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# 3. F1 Score Progress\n",
    "axes[0, 2].plot(epochs, training_history['val_f1'], 'm-d', label='Validation F1', linewidth=2)\n",
    "axes[0, 2].set_title('F1-Score Progress', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('F1-Score')\n",
    "axes[0, 2].set_ylim(0, 1)\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confusion Matrix Heatmap\n",
    "cm = evaluation_results['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Continental', 'Analytic'], \n",
    "           yticklabels=['Continental', 'Analytic'],\n",
    "           ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Predicted')\n",
    "axes[1, 0].set_ylabel('Actual')\n",
    "\n",
    "# 5. Per-Class Performance Metrics\n",
    "class_names = ['Continental', 'Analytic']\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "continental_scores = [evaluation_results['precision_per_class'][0], \n",
    "                     evaluation_results['recall_per_class'][0], \n",
    "                     evaluation_results['f1_per_class'][0]]\n",
    "analytic_scores = [evaluation_results['precision_per_class'][1], \n",
    "                  evaluation_results['recall_per_class'][1], \n",
    "                  evaluation_results['f1_per_class'][1]]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 1].bar(x - width/2, continental_scores, width, label='Continental', color='skyblue')\n",
    "bars2 = axes[1, 1].bar(x + width/2, analytic_scores, width, label='Analytic', color='lightcoral')\n",
    "\n",
    "axes[1, 1].set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "# 6. Probability Distribution Analysis\n",
    "probabilities = np.array(evaluation_results['probabilities'])\n",
    "continental_probs = probabilities[:, 0]\n",
    "analytic_probs = probabilities[:, 1]\n",
    "\n",
    "axes[1, 2].hist(continental_probs, bins=20, alpha=0.7, label='Continental Predictions', color='skyblue', density=True)\n",
    "axes[1, 2].hist(analytic_probs, bins=20, alpha=0.7, label='Analytic Predictions', color='lightcoral', density=True)\n",
    "axes[1, 2].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Probability Score')\n",
    "axes[1, 2].set_ylabel('Density')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a summary statistics table\n",
    "print(\"\\\\n📊 TRAINING SUMMARY STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Training Loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {training_history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final F1-Score: {training_history['val_f1'][-1]:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(training_history['val_accuracy']):.4f}\")\n",
    "print(f\"Best F1-Score: {max(training_history['val_f1']):.4f}\")\n",
    "\n",
    "# Model complexity info\n",
    "total_params = sum(p.numel() for p in classifier.model.parameters())\n",
    "trainable_params = sum(p.numel() for p in classifier.model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\\\n🔧 MODEL STATISTICS\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen Parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"Model Size: ~{total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Training data statistics\n",
    "print(f\"\\\\n📚 DATA STATISTICS\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Training Samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_loader.dataset)}\")\n",
    "print(f\"Total Samples: {len(train_loader.dataset) + len(val_loader.dataset)}\")\n",
    "print(f\"Batch Size: {train_loader.batch_size}\")\n",
    "print(f\"Total Training Batches: {len(train_loader)}\")\n",
    "print(f\"Total Validation Batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3134fd8",
   "metadata": {},
   "source": [
    "## 12. Save and Load Model Functions\n",
    "\n",
    "Finally, let's implement functions to save our trained model and load it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = \"philosophy_bert_classifier.pth\"\n",
    "\n",
    "print(\"💾 Saving trained model...\")\n",
    "try:\n",
    "    classifier.save_model(model_save_path)\n",
    "    \n",
    "    # Save training history and evaluation results\n",
    "    training_data = {\n",
    "        'training_history': training_history,\n",
    "        'evaluation_results': {\n",
    "            'accuracy': evaluation_results['accuracy'],\n",
    "            'precision_per_class': evaluation_results['precision_per_class'].tolist(),\n",
    "            'recall_per_class': evaluation_results['recall_per_class'].tolist(),\n",
    "            'f1_per_class': evaluation_results['f1_per_class'].tolist(),\n",
    "            'confusion_matrix': evaluation_results['confusion_matrix'].tolist(),\n",
    "            'class_names': class_names\n",
    "        },\n",
    "        'model_info': {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'training_samples': len(train_loader.dataset),\n",
    "            'validation_samples': len(val_loader.dataset)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(\"training_results.json\", \"w\") as f:\n",
    "        json.dump(training_data, f, indent=2)\n",
    "    \n",
    "    print(\"✅ Model and training results saved successfully!\")\n",
    "    print(f\"   Model: {model_save_path}\")\n",
    "    print(f\"   Training data: training_results.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving model: {e}\")\n",
    "\n",
    "# Demonstrate model loading\n",
    "print(f\"\\\\n🔄 Testing model loading...\")\n",
    "\n",
    "# Create a new classifier instance\n",
    "new_classifier = PhilosophyClassifier()\n",
    "\n",
    "try:\n",
    "    # Load the saved model\n",
    "    new_classifier.load_model(model_save_path)\n",
    "    \n",
    "    # Test that the loaded model works\n",
    "    test_text = \"The dialectical movement of consciousness reveals the self-negating nature of absolute knowledge.\"\n",
    "    result = new_classifier.predict_single(test_text)\n",
    "    \n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    print(f\"   Test prediction: {max(result, key=result.get)} ({max(result.values()):.1%} confidence)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "\n",
    "# Create a simple function for end users\n",
    "def load_philosophy_classifier(model_path=\"philosophy_bert_classifier.pth\"):\n",
    "    \"\"\"\n",
    "    Convenience function to load a trained philosophy classifier.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model file\n",
    "        \n",
    "    Returns:\n",
    "        Loaded PhilosophyClassifier instance\n",
    "    \"\"\"\n",
    "    classifier = PhilosophyClassifier()\n",
    "    classifier.load_model(model_path)\n",
    "    return classifier\n",
    "\n",
    "def classify_text(text, model_path=\"philosophy_bert_classifier.pth\"):\n",
    "    \"\"\"\n",
    "    Quick function to classify a single text.\n",
    "    \n",
    "    Args:\n",
    "        text: Philosophical text to classify\n",
    "        model_path: Path to the saved model file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with classification results\n",
    "    \"\"\"\n",
    "    classifier = load_philosophy_classifier(model_path)\n",
    "    return predict_philosophical_style(text, classifier)\n",
    "\n",
    "print(f\"\\\\n🎯 USAGE EXAMPLES FOR END USERS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"# Load a trained model:\")\n",
    "print(\"classifier = load_philosophy_classifier('philosophy_bert_classifier.pth')\")\n",
    "print()\n",
    "print(\"# Classify a single text:\")\n",
    "print(\"result = classify_text('Your philosophical text here')\")\n",
    "print(\"display_prediction(result)\")\n",
    "print()\n",
    "print(\"# Or use the loaded classifier directly:\")\n",
    "print(\"result = predict_philosophical_style('Your text', classifier)\")\n",
    "\n",
    "# Create a final summary\n",
    "print(f\"\\\\n\\\\n🎓 PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Successfully created a BERT-based philosophical text classifier\")\n",
    "print(\"✅ Trained on Continental vs Analytic philosophy samples\")\n",
    "print(f\"✅ Achieved {evaluation_results['accuracy']:.1%} accuracy on validation data\")\n",
    "print(\"✅ Model saved and ready for deployment\")\n",
    "print()\n",
    "print(\"📚 The classifier can distinguish between:\")\n",
    "print(\"   🏛️  Continental Philosophy (Heidegger, Sartre, Derrida, etc.)\")\n",
    "print(\"   🔬 Analytic Philosophy (Russell, Quine, Kripke, etc.)\")\n",
    "print()\n",
    "print(\"🚀 Ready to classify your own philosophical texts!\")\n",
    "print(\"   Use the prediction functions above to analyze any text.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
